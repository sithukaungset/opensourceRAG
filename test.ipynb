{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-embeddings-huggingface in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (0.2.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.24.5)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-embeddings-huggingface) (0.10.66)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.1 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-embeddings-huggingface) (3.0.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.12.2)\n",
      "Requirement already satisfied: aiohttp in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.10.3)\n",
      "Requirement already satisfied: minijinja>=1.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.0.32)\n",
      "Requirement already satisfied: dataclasses-json in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.0.8)\n",
      "Requirement already satisfied: httpx in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.3)\n",
      "Requirement already satisfied: nltk>=3.8.1 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.8.1)\n",
      "Collecting numpy<2.0.0 (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: openai>=1.1.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.41.0)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.2.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (10.4.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.7.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.16.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (4.44.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.4.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.14.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.3.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.9.4)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from nltk>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from nltk>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from nltk>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2024.7.24)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.3.1)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.0.3)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.13.2)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (72.1.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.4.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.19.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2024.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.5.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n",
      "Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.1\n",
      "    Uninstalling numpy-2.0.1:\n",
      "      Successfully uninstalled numpy-2.0.1\n",
      "Successfully installed numpy-1.26.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index-llms-llama-cpp 0.1.4 requires llama-cpp-python<0.3.0,>=0.2.32, but you have llama-cpp-python 0.1.65 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-llms-llama-cpp in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (0.1.4)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting llama-cpp-python<0.3.0,>=0.2.32 (from llama-index-llms-llama-cpp)\n",
      "  Using cached llama_cpp_python-0.2.88-cp312-cp312-win_amd64.whl\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-llms-llama-cpp) (0.10.66)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (1.26.4)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (3.1.4)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.10.3)\n",
      "Requirement already satisfied: dataclasses-json in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2024.6.1)\n",
      "Requirement already satisfied: httpx in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.3)\n",
      "Requirement already satisfied: nltk>=3.8.1 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.8.1)\n",
      "Requirement already satisfied: openai>=1.1.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.41.0)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.2.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (10.4.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (4.66.5)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.3.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.9.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python<0.3.0,>=0.2.32->llama-index-llms-llama-cpp) (2.1.5)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from nltk>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from nltk>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from nltk>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2024.7.24)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.3.1)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (3.21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2024.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (24.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-llama-cpp) (1.16.0)\n",
      "Installing collected packages: llama-cpp-python\n",
      "  Attempting uninstall: llama-cpp-python\n",
      "    Found existing installation: llama_cpp_python 0.1.65\n",
      "    Uninstalling llama_cpp_python-0.1.65:\n",
      "      Successfully uninstalled llama_cpp_python-0.1.65\n",
      "Successfully installed llama-cpp-python-0.2.88\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-embeddings-huggingface\n",
    "%pip install llama-index-llms-llama-cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.llms.llama_cpp.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (0.10.65)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.3.0,>=0.1.4 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index) (0.2.9)\n",
      "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index) (0.1.13)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.65 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index) (0.10.66)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index) (0.1.11)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.2.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index) (0.2.7)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index) (0.9.48.post2)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.27 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index) (0.1.29)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index) (0.1.9)\n",
      "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index) (0.1.7)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index) (0.1.3)\n",
      "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index) (0.1.33)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.1.2 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index) (0.1.6)\n",
      "Requirement already satisfied: openai>=1.14.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-agent-openai<0.3.0,>=0.1.4->llama-index) (1.41.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.65->llama-index) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (3.10.3)\n",
      "Requirement already satisfied: dataclasses-json in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (2024.6.1)\n",
      "Requirement already satisfied: httpx in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (3.3)\n",
      "Requirement already satisfied: nltk>=3.8.1 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (3.8.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (2.2.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (10.4.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.65->llama-index) (1.16.0)\n",
      "Requirement already satisfied: llama-cloud>=0.0.11 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-indices-managed-llama-cloud>=0.2.0->llama-index) (0.0.13)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.3.1)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.4.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-index-readers-llama-parse>=0.1.2->llama-index) (0.4.9)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.65->llama-index) (2.3.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.65->llama-index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.65->llama-index) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.65->llama-index) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.65->llama-index) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.65->llama-index) (1.9.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.6)\n",
      "Requirement already satisfied: pydantic>=1.10 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud>=0.2.0->llama-index) (2.8.2)\n",
      "Requirement already satisfied: anyio in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.65->llama-index) (4.4.0)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.65->llama-index) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.65->llama-index) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.65->llama-index) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.65->llama-index) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.65->llama-index) (0.14.0)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from nltk>=3.8.1->llama-index-core<0.11.0,>=0.10.65->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from nltk>=3.8.1->llama-index-core<0.11.0,>=0.10.65->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from nltk>=3.8.1->llama-index-core<0.11.0,>=0.10.65->llama-index) (2024.7.24)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama-index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama-index) (0.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.65->llama-index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.65->llama-index) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.65->llama-index) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.11.0,>=0.10.65->llama-index) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.65->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.65->llama-index) (3.21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.65->llama-index) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.65->llama-index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.65->llama-index) (2024.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.65->llama-index) (24.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from pydantic>=1.10->llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud>=0.2.0->llama-index) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from pydantic>=1.10->llama-cloud>=0.0.11->llama-index-indices-managed-llama-cloud>=0.2.0->llama-index) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\envs\\llama-cpp-env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.65->llama-index) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = \"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q2_K.gguf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading url https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q2_K.gguf to path C:\\Users\\MZC01-SITHU\\AppData\\Local\\llama_index\\models\\llama-2-7b-chat.Q2_K.gguf\n",
      "total size (MB): 2825.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2696it [04:28, 10.04it/s]                          \n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from C:\\Users\\MZC01-SITHU\\AppData\\Local\\llama_index\\models\\llama-2-7b-chat.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 2.63 GiB (3.35 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  2694.32 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3904\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1952.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1952.00 MiB, K (f16):  976.00 MiB, V (f16):  976.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   283.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '10', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    model_url=model_url,\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=None,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 1},\n",
    "    # transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llama-cpp-env\\Lib\\site-packages\\llama_cpp\\llama.py:1130: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n",
      "\n",
      "llama_print_timings:        load time =    8832.66 ms\n",
      "llama_print_timings:      sample time =       9.19 ms /   207 runs   (    0.04 ms per token, 22536.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8832.21 ms /    80 tokens (  110.40 ms per token,     9.06 tokens per second)\n",
      "llama_print_timings:        eval time =   36677.09 ms /   206 runs   (  178.04 ms per token,     5.62 tokens per second)\n",
      "llama_print_timings:       total time =   45753.71 ms /   286 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course! Here is a poem about cats and dogs:\n",
      "Cats and dogs, so different yet the same,\n",
      "Both furry friends, with hearts that play the game.\n",
      "Cats are sly, with eyes so bright,\n",
      "Dogs are loyal, with tails that wag with delight.\n",
      "Cats prowl through the night, with stealthy grace,\n",
      "Dogs chase their tails, with joyful face.\n",
      "Cats curl up tight, in cozy balls,\n",
      "Dogs stretch out long, with paws that sprawl.\n",
      "Cats purr and pur, with contented sighs,\n",
      "Dogs wag their tails, with joyful surprise.\n",
      "Both cats and dogs, they bring us joy,\n",
      "With their playful ways, they make our hearts employ.\n",
      "So here's to cats and dogs, our furry friends,\n",
      "Who bring us laughter, and endless fun.\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\"Hello! Can you tell me a poem about cats and dogs?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 62 prefix-match hit, remaining 14 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course, I'd be happy to help! Here's a poem about fast cars:\n",
      "Racing down the highway, wind in my hair\n",
      "Adrenaline pumping, heart beating fair\n",
      "Fast cars, sleek and strong\n",
      "Taking me where I want to go\n",
      "\n",
      "The engine roars, the tires squeal\n",
      "As I speed past, the world reveals\n",
      "A blur of colors, a kaleidoscope\n",
      "As I drive, my spirit soars\n",
      "\n",
      "The road unwinds, a never-ending stream\n",
      "Of asphalt and adventure, a dream\n",
      "Fast cars, a thrill and a delight\n",
      "Taking me on a journey through the night\n",
      "\n",
      "So here I'll stay, behind the wheel\n",
      "With the wind in my hair, the thrill to feel\n",
      "Fast cars, a joy to behold\n",
      "Taking me where I want to go, bold\n",
      "\n",
      "I hope you enjoy the poem! Let me know if you have any other requests."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8832.66 ms\n",
      "llama_print_timings:      sample time =      10.74 ms /   217 runs   (    0.05 ms per token, 20201.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     740.56 ms /    14 tokens (   52.90 ms per token,    18.90 tokens per second)\n",
      "llama_print_timings:        eval time =   41913.33 ms /   216 runs   (  194.04 ms per token,     5.15 tokens per second)\n",
      "llama_print_timings:       total time =   43147.61 ms /   230 tokens\n"
     ]
    }
   ],
   "source": [
    "response_iter = llm.stream_complete(\"Can you write me a poem about fast cars?\")\n",
    "for response in response_iter:\n",
    "    print(response.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (2382184276.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[10], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    model_path= 'C:\\Users\\MZC01-SITHU\\Desktop\\opensourceRAG\\models\\llama-2-7b-chat.Q2_K.gguf',\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    #model_url=model_url,\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path= 'models\\llama-2-7b-chat.Q2_K.gguf',\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 1},\n",
    "    # transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-cpp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
